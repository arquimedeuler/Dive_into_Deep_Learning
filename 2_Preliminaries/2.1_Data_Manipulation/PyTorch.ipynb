{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be361b5b-420d-4173-b792-33d1382b8b25",
   "metadata": {},
   "source": [
    "# 2.1. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ec69a-f1f3-4ddd-8df3-e5a373d8ad45",
   "metadata": {},
   "source": [
    "# 2.1.1. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5ae4d-c7d4-496f-8334-8cac5107f4b9",
   "metadata": {},
   "source": [
    "To start, we import the PyTorch library. Note that the package name is torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f5d87a-9ac9-4a74-9f02-b149cd35af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a4b9fa-677b-4201-8710-f964f97ddf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3e180-f95e-4958-8347-fa3dfe8f13b3",
   "metadata": {},
   "source": [
    "Each of these values is called an element of the tensor. The tensor _x_ contains 12 elements. We can inspect the total number of elements in a tensor via its ```numel``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb4e7f9-0109-4dbf-a73e-45c89b4f5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46730577-fc50-4132-9122-484e052c07e4",
   "metadata": {},
   "source": [
    "We can access a tensorâ€™s shape (the length along each axis) by inspecting its ```shape``` attribute. Because we are dealing with a vector here, the shape contains just a single element and is identical to the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f8786d-f3a8-452a-a686-343603b01cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e749763-0c35-4c00-a750-73d27dceec7d",
   "metadata": {},
   "source": [
    "We can change the shape of a tensor without altering its size or values, by invoking ```reshape```. For example, we can transform our vector x whose shape is (12,) to a matrix X with shape (3, 4). This new tensor retains all elements but reconfigures them into a matrix. Notice that the elements of our vector are laid out one row at a time and thus x[3] == X[0, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b50918-2677-4091-b4ca-0537fff6e478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06347fb1-21b0-45c3-a2eb-0024589d6980",
   "metadata": {},
   "source": [
    "Practitioners often need to work with tensors initialized to contain all 0s or 1s. We can construct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the ```zeros``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b7ad66-f073-4560-b94d-7a7adbeb0573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9230574-01dc-4d81-b1d8-cb9b7577d9f2",
   "metadata": {},
   "source": [
    "Similarly, we can create a tensor with all 1s by invoking ```ones```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591fe1fc-c480-407d-933f-3ac30273145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb8e39-4b27-485a-9b90-bf6ec4b62584",
   "metadata": {},
   "source": [
    "We often wish to sample each element randomly (and independently) from a given probability distribution. For example, the parameters of neural networks are often initialized randomly. The following snippet creates a tensor with elements drawn from a standard Gaussian (normal) distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28457086-fdf0-4d4b-add8-a9a3bbfdb818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3180,  0.2145, -0.9253, -0.5148],\n",
       "        [ 0.0696, -0.5682,  1.6224, -0.6974],\n",
       "        [-1.0287, -1.4168, -0.5643,  0.0030]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fb5bc-827b-46a5-a532-a108cc00c969",
   "metadata": {},
   "source": [
    "Finally, we can construct tensors by supplying the exact values for each element by supplying (possibly nested) Python list(s) containing numerical literals. Here, we construct a matrix with a list of lists, where the outermost list corresponds to axis 0, and the inner list corresponds to axis 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790d5cc2-0e55-41e8-b2ec-a9315016be36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ed6de-c92f-430e-b30d-f7e5abaa27f5",
   "metadata": {},
   "source": [
    "## 2.1.2. Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5bec972-15d4-45a7-8075-09a19fed40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c3748-5473-40ad-b34d-8ed0b345aed9",
   "metadata": {},
   "source": [
    "Beyond reading them, we can also write elements of a matrix by specifying indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e94a471-cd33-4755-9a16-2e1213606c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe7955-9b4f-4463-886c-62c04640345c",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we apply the indexing on the left-hand side of the assignment operation. For instance, [:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this also works for vectors and for tensors of more than two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519b848e-6db8-4a07-8c58-58528c71a5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b035507-5109-4a4e-8619-a80bc3b40bd0",
   "metadata": {},
   "source": [
    "## 2.1.3. Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1ba08-b6b7-4add-960b-73db0f864095",
   "metadata": {},
   "source": [
    "In mathematical notation, we denote such _unary_ scalar operators (taking one input) by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    ". This just means that the function maps from any real number onto some other real number. Most standard operators, including unary ones like$e^x$\r",
    " , can be applied elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cadbcfa-e69a-4f2c-85f3-192747b414d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n",
       "        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n",
       "         22026.4648,  59874.1406])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798243bb-9d27-42e6-951e-cc59a956f5de",
   "metadata": {},
   "source": [
    "Likewise, we denote binary scalar operators, which map pairs of real numbers to a (single) real number via the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$. Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ of the same shape, and a binary operator $f$, we can produce a vector $\\mathbf{c} = F\\left(\\mathbf{u}, \\mathbf{v}\\right)$ by setting $c_i \\leftarrow f\\left(u_i, v_i \\right)$, for all $i$, where $c_i$, $u_i$ and $v_i$ are the $i^\\text{th}$ elements of vectors $\\mathbf{c}$, $\\mathbf{u}$ and $\\mathbf{v}$. Here, we produced the vector-valued $F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ by _lifting_ the scalar function to an elementwise vector operation. The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been _lifted_ to elementwise operations for identically-shaped tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72045347-a05c-4391-b77a-cef250293116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd62c2-6e83-4ea0-8299-a201de89b7d7",
   "metadata": {},
   "source": [
    "We can also concatenate multiple tensors, stacking them end-to-end to form a larger one. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along rows (axis 0) instead of columns (axis 1). y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d65f73c-a639-4797-8c5d-0029d0fd55c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype = torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim = 0), torch.cat((X, Y), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899f35d-d566-4061-85e3-6b013ebbc274",
   "metadata": {},
   "source": [
    "Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position i, j, if X[i, j] and Y[i, j] are equal, then the corresponding entry in the result takes value 1, otherwise it takes value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6267e270-0b1e-43b4-921a-c2e65dbfdb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff31c4-566c-42a0-8ac5-2ef11f757e89",
   "metadata": {},
   "source": [
    "Summing all the elements in the tensor yields a tensor with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb4c1b4-884a-440e-8c64-5ffd2338eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f610387-fe4b-4327-a722-fead0aee0e1b",
   "metadata": {},
   "source": [
    "## 2.1.4. Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f555cf1-6e45-4094-b4fb-114b5ff73f1e",
   "metadata": {},
   "source": [
    "Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the broadcasting mechanism. Broadcasting works according to the following two-step procedure: (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0bf8acf-20f7-4922-9a32-eaa267a6507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42018aa3-9bec-4377-8611-d7687adb584f",
   "metadata": {},
   "source": [
    "Since a and b are $3\\times 1$ and $1\\times 2$ matrices, respectively, their shapes do not match up. Broadcasting produces a larger $3\\times 2$ matrix by replicating matrix a along the columns and matrix b along the rows before adding them elementwise.\n",
    "\n",
    "$$ a = \\begin{pmatrix}\n",
    "0\\\\\n",
    "1 \\\\ \n",
    "2 \n",
    "\\end{pmatrix}, \\quad b = \\begin{pmatrix}\n",
    "0, 1 \\end{pmatrix}$$\n",
    "\n",
    "$$ a + b \\rightarrow \\begin{pmatrix}\n",
    "0, 0\\\\\n",
    "1, 1 \\\\ \n",
    "2, 2 \n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "0, 1\\\\\n",
    "0, 1 \\\\ \n",
    "0, 1 \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "0, 1\\\\\n",
    "1, 2 \\\\ \n",
    "2, 3 \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d429241f-e6c2-4f3e-a6f6-9a14c78af26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2551d05-3173-4dfa-b8ea-daa0bf5f8f71",
   "metadata": {},
   "source": [
    "## 2.1.5. Saving Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0bb43-b0c8-463e-acca-976d311106a8",
   "metadata": {},
   "source": [
    "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. We can demonstrate this issue with Pythonâ€™s id() function, which gives us the exact address of the referenced object in memory. Note that after we run Y = Y + X, id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then points Y to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a98a9b-2170-4204-b98c-548ab1e4a69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2917655757184"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fae6f5e-0bd6-44f1-a43b-4046bb952997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d2d37-7b8e-48c3-87ac-b22ddf380e20",
   "metadata": {},
   "source": [
    "We can assign the result of an operation to a previously allocated array Y by using slice notation: Y[:] = ```<expression>```. To illustrate this concept, we overwrite the values of tensor Z, after initializing it, using zeros_like, to have the same shape as Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcbd3987-6ebe-4140-be31-22ed75703c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2917655761744\n",
      "id(Z): 2917655761744\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print(f\"id(Z): {id(Z)}\")\n",
    "Z[:] = X + Y\n",
    "print(f\"id(Z): {id(Z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff622b-574e-4557-8544-2369ed8a38c8",
   "metadata": {},
   "source": [
    "If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defc338a-bc0d-423b-b706-015d547d23fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed2698-56be-4041-a8bd-387c8e5af8be",
   "metadata": {},
   "source": [
    "## 2.1.6. Conversion to Other Python Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59207e40-cbfa-46ed-b1e3-550caedd7760",
   "metadata": {},
   "source": [
    "Converting to a NumPy tensor (ndarray), or vice versa, is easy. The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9297d668-76c3-42dd-b32b-61d177f20100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3e2d6-e940-4ff1-a9d7-46f1c56a726c",
   "metadata": {},
   "source": [
    "To convert a size-1 tensor to a Python scalar, we can invoke the ```item``` function or Pythonâ€™s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a335730-43c9-48b6-b58c-d74a2894d934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e4960-fdca-48f1-8531-d459042a80d7",
   "metadata": {},
   "source": [
    "## 2.1.7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4b32e-4852-471a-aaf1-c78a42466dcc",
   "metadata": {},
   "source": [
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a0b0e-b75e-4015-9df8-ec277c9d2745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
